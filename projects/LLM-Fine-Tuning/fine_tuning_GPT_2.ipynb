{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
      },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc0JOa3nVvN8",
        "outputId": "a5ac0fb4-ddbd-4f2b-bf43-43d72d4a0181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
            "Collecting trl\n",
            "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
            "Downloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.26.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Since it printed cuda, it means in your PyTorch code running on Google Colab will now execute on the GPU instead of the CPU.\n",
        "\n",
        "*   Any tensor or model you send to device (with .to(device)) will be stored in GPU memory\n",
        "\n",
        "*   Training (including LLM fine-tuning) will run using CUDA acceleration\n",
        "\n",
        "*   Computations will be much faster, which is essential for AI/data roles you’re targeting at companies using deep learning or GenAI"
      ],
      "metadata": {
        "id": "PRUaFUotXXr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LeIjKKDLWYgB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQdkPw95W0ac",
        "outputId": "c2704ab3-515d-428c-9917-a043b57f97d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This dataset will allow us to fine tune GPT2 into a model that can perform Q&A (chatbot)"
      ],
      "metadata": {
        "id": "1maIkLp0YK0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2 is in its pretrained state. GPT2 is not as advanced ChatGPT you can't talk to it now (just a text completion model)"
      ],
      "metadata": {
        "id": "wFtgBuxLYlk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "DATASET_NAME = \"mlabonne/guanaco-llama2-1k\"\n",
        "dataset = load_dataset(DATASET_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "f3758bc86ee74122ac302d801387e538",
            "1c2828dd3c9f443b9cc26709b71de6b6",
            "ca11305197c540e08e020ea785aab9d7",
            "581fc94cd4f14ba0b3624ba6f2dc4df8",
            "ddcde9b05c33463eba1714602d27dbd9",
            "bb9a4710f53a443db9b9cd9c98f5c6e0",
            "35ad03220b974260af17a046a681dacc",
            "7016a13da1e244e5b00a40b6faf96dc3",
            "1a674963975b4d688d71e6de4bce8e96",
            "0a320d6d07ea4a54b787d453794565dc",
            "2e01c398b2264562805635ddf4cf824d",
            "d008a7ff336242cdad467f14118003b5",
            "12490a732a954074bf043eacdb638d07",
            "be0f480ebfb64e049083bb2f2cedecb6",
            "b8ea51aab40c43bcbdf9001995651a40",
            "5ddfdb154cb448048fe2620914771476",
            "0d6e54661af24cd9b129eb25d6f08596",
            "5b430db7fe8c41ce8d69c6e72cddf414",
            "e2378e6f3b2e4e7cb1fe837a1acd430e",
            "3960534c86d3437dbd29b3445054106f",
            "a285e3cc72a6428c93e67c28972672d0",
            "12ef5d3b80a84a958fb7ad2560242980",
            "58c0023c9c1f482c9c12f63e2427083a",
            "125a8e3b659841bd8cbd1731b3f17823",
            "d483404a2d0e42afa685778e2f4318a2",
            "b72be69b96e94573baed16812c7b9120",
            "913b7a863b6c49fcbfaae8bae47cb5a6",
            "6e1be00c141c448994e102e8abfe09cd",
            "3b61c056d5b348bfbcab4dc6f59225f7",
            "433e77e4006f4339904c3d2eec78dd75",
            "0bd519024de341669b92a675fed85166",
            "23f3b56e91f74a58b21560704bcd45c1",
            "e475554c70d3485e9eae20abc226ce86"
          ]
        },
        "id": "hNFh0tzEXNqy",
        "outputId": "ac160660-ba6d-46e8-da6c-3c6ab313b5b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3758bc86ee74122ac302d801387e538"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001-9ad84bb9cf65a4(…):   0%|          | 0.00/967k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d008a7ff336242cdad467f14118003b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58c0023c9c1f482c9c12f63e2427083a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_AxtpyuYCw0",
        "outputId": "da4b0724-caa1-4508-c61e-bff73ee5a69a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset['train']\n",
        "print(training_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU211awcYsOb",
        "outputId": "eb068a03-73e8-4ebb-b39a-458de12321bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 1000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model will learn langauges other than english"
      ],
      "metadata": {
        "id": "oyM56aTla29k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoJwoGKmZ89r",
        "outputId": "2c455bfa-0a70-479e-de3e-203fd9de2adf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "format in question and answer pairs"
      ],
      "metadata": {
        "id": "EJ3q1RJKamup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I_NS4m-advN",
        "outputId": "b4acf7db-f3e6-4ab7-c160-e126e838c39d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rPC60MBaF_7",
        "outputId": "fc6ea3c1-7c77-433b-d3c4-f0b6f5d8e5f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>',\n",
              "  '<s>[INST] Самый великий человек из всех живших на планете? [/INST] Для начала нужно выбрать критерии величия человека. Обычно великим называют человека, который внес большой вклад в общество или сильно выделялся на фоне других в своем деле.\\n\\nНапример, Иосифа Бродского считают великим поэтом, а Иммануила Канта — великим философом. Александр Македонский, известный тем, что собрал в свои владения огромную империю (включавшую Македонию, Грецию, Персию, Египет), в историографии носит имя Александр Великий. Для христиан, скорее всего, самым великим человеком жившим на земле был Иисус Христос, так как он совершил множество благих деяний и совершил подвиг ради человечества. \\n\\nПри этом, когда мы выдвигаем одну личность на роль великого человека, сразу же находится множество людей, не согласных с этим. Того же Иосифа Бродского, хоть он и получил престижную Нобелевскую премию, некоторые люди считают графоманом и посредственным поэтом. \\n\\nВ целом, кого считать великим — это самостоятельный выбор каждого человека, который можно сделать только соотнося его со своими личными ценностями и представлениями о красивом, правильном, хорошем. </s>',\n",
              "  '<s>[INST] Compose a professional email with the following points:\\n\\nMe chinese cook 10 years\\nGood good cook\\nPeople love my food\\nCan you hire me? [/INST] Thanks for the information. Unfortunately, your initial message does not contain enough details or structure to compose a compelling professional email to a potential employer. Here are some suggestions for how we can improve the message:\\n\\n1) Let\\'s first clarify the type of cooking position you are interested in. Are you looking for a job as a line cook, sous chef, executive chef, or something else? This will help determine how to emphasize relevant experience and skills.\\n\\n2) What specific experience do you have? For example, \"Over 10 years of experience preparing authentic Sichuan and Cantonese cuisine.\" Highlight any relevant skills, like knife skills, cooking techniques, menu development, etc. Mention the types of cuisine you specialize in.\\n\\n3) Have you worked in a professional kitchen before? If so, mention the name and type of restaurant (upscale, casual dining, etc.). Emphasize relevant responsibilities and accomplishments. If not, that\\'s fine too, just be clear that you\\'re looking to transition into a professional kitchen and highlight related skills and passion for cooking.\\n\\n4) Do you have any certifications or training? Note any culinary degrees, certificates, or other credentials. Even informal training can show your dedication to the craft.\\n\\n5) Are you looking for full-time, part-time, temporary, or contract work? Be specific about the type of opportunity you\\'re seeking.\\n\\nWith the answers to these questions, we can craft an email in a standard business letter format. The opening can express your interest in the position and summarize your relevant experience and qualifications. The body can go into more depth on your experience, skills, and goals. The closing can reiterate your interest and mention that you\\'d welcome the opportunity to further discuss the role.\\n\\nDoes this help? Let me know if you would like help crafting the full email. I\\'m happy to assist you in composing a professional and compelling message to potential employers. </s>',\n",
              "  '<s>[INST] ¿Qué juegos me recomendarías si me ha gustado Undertale? [/INST] Deltarune: Este es un juego creado por el mismo desarrollador de Undertale, Toby Fox. Al igual que Undertale, Deltarune combina elementos de RPG con una historia divertida y personajes memorables.\\n\\nEarthBound: Este es un juego clásico de RPG que se ha convertido en un cult classic en todo el mundo. Al igual que Undertale, EarthBound combina elementos de comedia y drama en una historia única y memorable.\\n\\nHollow Knight: Este es un juego de aventura y metroidvania que cuenta con un mundo oscuro y misterioso, así como personajes interesantes y una banda sonora memorable.\\n\\nHyper Light Drifter: Este es un juego de aventura y acción que cuenta con un mundo hermoso y misterioso, así como combates desafiantes y una banda sonora impactante.\\n\\nOri and the Blind Forest: Este es un juego de aventura y plataformas que cuenta con un mundo hermoso y una historia conmovedora.\\n\\nEstos son solo algunos ejemplos de juegos similares a Undertale. Cualquiera de estos juegos podría ser una buena opción para aquellos que disfrutaron de Undertale y buscan algo similar. </s><s>[INST] EarthBound suena divertido, puedes contarme más sobre su historia [/INST] \"EarthBound\" es un juego de rol clásico de la consola Super Nintendo Entertainment System (SNES) que fue desarrollado y publicado por Nintendo en 1995. La historia del juego sigue a un joven llamado Ness y a sus amigos en un viaje épico para salvar el mundo.\\n\\nEl juego comienza en la ciudad de Onett, donde Ness es despertado por un extraño sueño. Después de investigar, descubre que un asteroide ha caído cerca de su casa y decide investigar. Allí, Ness encuentra a un extraterrestre llamado Buzz Buzz que le da la misión de salvar el mundo de un ser maligno llamado Giygas.\\n\\nA lo largo del juego, Ness viaja por todo el mundo, luchando contra enemigos y haciendo amigos con otros personajes. Algunos de sus compañeros de aventura incluyen a Paula, una telepata, y Jeff, un inventor prodigio. Juntos, deben enfrentarse a una serie de desafíos y desafiantes jefes finales para cumplir su misión y salvar al mundo.\\n\\n\"EarthBound\" se destaca por su estilo único, que combina elementos de juegos de rol clásicos con una ambientación surreal y humorística. También es conocido por su banda sonora inolvidable y sus personajes carismáticos. Aunque el juego no fue un gran éxito en su lanzamiento, ha ganado un culto de seguidores a lo largo de los años y es considerado por muchos como un clásico de los juegos de rol. </s>',\n",
              "  '<s>[INST] Cual es el desarrollo motor de niño/a de 0 a 6 años [/INST] El desarrollo motor de un niño/a de 0 a 6 años puede ser dividido en varios hitos importantes, que suceden en un orden predecible pero que pueden variar en el tiempo que tardan en alcanzarse. Algunos de estos hitos incluyen:\\n\\nDesde el nacimiento hasta los 3 meses: Los bebés tienen movimientos reflejos y pueden levantar la cabeza brevemente cuando están boca abajo. Comienzan a seguir objetos con la mirada y pueden empezar a sostener objetos con las manos.\\nDe 3 a 6 meses: Los bebés empiezan a rodar de lado a lado y a sentarse con apoyo. También pueden empezar a gatear y a hacer movimientos más coordinados con las manos.\\nDe 6 a 9 meses: Los bebés empiezan a sentarse sin apoyo y pueden comenzar a gatear hacia adelante. También pueden sostener objetos con la mano y pueden empezar a transferir objetos de una mano a otra.\\nDe 9 a 12 meses: Los bebés pueden empezar a caminar con apoyo y a levantarse sin ayuda. También pueden empezar a gatear hacia atrás y a gatear en posición cuadrúpeda. Además, pueden comenzar a utilizar la pincer grip (utilizando el dedo índice y el pulgar) para agarrar objetos.\\nDe 1 a 2 años: Los niños/as pueden caminar sin ayuda y empezar a correr y saltar. También pueden empezar a subir escaleras, utilizando ambos pies para subir un escalón a la vez. Además, pueden empezar a usar utensilios para comer y a realizar dibujos más precisos con un lápiz o crayón.\\nDe 2 a 3 años: Los niños/as pueden subir escaleras utilizando un pie por escalón. También pueden empezar a pedalear en una bicicleta y saltar en un pie. Además, pueden empezar a vestirse y desvestirse solos, y a construir torres y estructuras simples con bloques.\\nDe 3 a 6 años: Los niños/as pueden saltar y correr con mayor coordinación y equilibrio, y pueden comenzar a realizar actividades físicas más complejas como nadar y jugar a juegos deportivos. También pueden comenzar a escribir y dibujar letras y números, y a realizar tareas más complejas como cortar con tijeras y manipular pequeños objetos.\\nEs importante tener en cuenta que estos hitos son solo una guía general y que cada niño/a se desarrolla a su propio ritmo. Algunos niños/as pueden alcanzar estos hitos más temprano o más tarde que otros, y eso es normal. Si tienes alguna preocupación sobre el desarrollo motor de tu hijo/a, siempre es recomendable hablar con un médico o especialista en desarrollo infantil. </s><s>[INST] Y el desarrollo del habla? [/INST] El desarrollo del habla en un niño o niña ocurre de manera progresiva y gradual, y puede variar en cada niño/a. A continuación, se presenta un resumen del desarrollo del habla en diferentes etapas:\\n\\n- De 0 a 6 meses: El bebé emite sonidos y vocalizaciones, como gorjeos, risas, llantos y gritos. Alrededor de los 4 meses, comienza a balbucear y a producir sonidos con mayor variedad de consonantes y vocales, como \"bababa\" o \"papapa\".\\n- De 6 a 12 meses: El bebé comienza a imitar los sonidos que escucha, y puede intentar decir algunas palabras simples como \"mamá\" o \"papá\". También puede usar gestos, como señalar objetos o levantar los brazos para que lo levanten.\\n- De 1 a 2 años: El niño o niña comienza a combinar palabras para formar frases simples, como \"más leche\" o \"quiero juguete\". A esta edad, el habla aún puede ser difícil de entender para personas que no están familiarizadas con el niño/a.\\n- De 2 a 3 años: El niño/a amplía su vocabulario y su capacidad para formar oraciones más complejas. También comienza a hacer preguntas, a utilizar pronombres y a emplear el plural. A esta edad, el habla se vuelve más clara y comprensible para las personas que no conviven diariamente con el niño/a.\\n- De 3 a 4 años: El niño/a comienza a utilizar frases más complejas y a contar historias con cierta coherencia. También es capaz de seguir instrucciones más detalladas y de responder a preguntas que requieren más reflexión.\\n- De 4 a 6 años: El niño/a sigue ampliando su vocabulario y su capacidad para formular oraciones complejas. También comienza a leer y escribir, lo que contribuye a su desarrollo lingüístico en general. A esta edad, el habla es fluida y bien estructurada.\\n\\nEs importante tener en cuenta que el desarrollo del habla y el lenguaje puede variar entre los niños y niñas, y que algunos pueden requerir ayuda adicional en caso de presentar retrasos o dificultades en su desarrollo. En caso de preocupación, se recomienda consultar a un especialista en el área de la salud infantil. </s>']}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When I want to fine tune a model in the future I will have block of code similar to this"
      ],
      "metadata": {
        "id": "RjKTLbHyNeWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of code below:\n",
        "\n",
        "Block 1:Loads a pretrained DistilGPT‑2 from the Transformers ecosystem onto the best available accelerator, and configures its tokenizer so sequences can be padded\n",
        "\n",
        "Block 2: takes the models text generation settings and updates them so it knows when to stop, how long to generate and how to sample the next token with creativity and filtering rules\n",
        "\n",
        "*   distilgpt2 is a model already pretrained (model weights are there), download model at pretrained state\n",
        "*   eos toekn = special marker to tell model this sentence has finished\n",
        "*   padding is adding extra blank filler toekns to make all text inputs the same length so the model can process them in batches without breaking\n",
        "*   gpt2 and chatbots are a type of LM callled casualLM because they generate one token at a time. Causal because toekns are predicted in a cause and effect way.\n",
        "1.   My\n",
        "2.   My Name\n",
        "3.   My name is\n",
        "4.   My name is Nathan\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BSrUw393PU_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"distilgpt2\"\n",
        "import transformers #now I can load HuggingFace models\n",
        "from transformers import AutoModelForCausalLM #an api\n",
        "from transformers import AutoTokenizer #an api\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map = \"auto\") #Downloads and loads the pretrained LLM. Auto move things to GPU if neccessary\n",
        "model.config.use_cache = True #model is passing in one token at a time such as My, my name, My name is as input to model each time we call it. we don't have to recompute the previous tokens, just cache the previous results.\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) #if model creator wrote their own special tokenizer code, allow colab to run it instead of only using default token rules\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = 'right' #add filler at the end of sentence, not beggining\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "generation_configuration = model.generation_config #copies models current generation settings into a variable so I can edit them\n",
        "generation_configuration.pad_token_id = tokenizer.eos_token_id # sets padding token to be eos\n",
        "generation_configuration.eos_token_id = tokenizer.eos_token_id #tells model this is the token that means stop generating here\n",
        "#the below have to do with how we sample the model\n",
        "generation_configuration.max_new_tokens = 1024 # GPT-2 has  factor in the past 1024 tokens in its repponse back to me\n",
        "generation_configuration.do_sample = True  #GPT added line\n",
        "generation_configuration.temperature = 0.7 #affects the quality and how diverse models outputs are, super bland or fun and varied. we dould divide every number by temperature in prob dist below\n",
        "generation_configuration.top_p = 0.9 # we only care about top p probabilities in our dist\n",
        "generation_configuration.tok_k = 20 #\n",
        "\n",
        "#[0.2, 0.23, 0.79, 0.5,..., 0.04, 0.16]"
      ],
      "metadata": {
        "id": "G2JedpEsaJqt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "46213848b23c4ec893afcb7b740cffeb",
            "afbc4c07086548809cbb3dce231b532a",
            "5a5ca41e80954944bdc4eceffb4f4587",
            "fbfd6098e2e94079a458bf6f569acfa9",
            "d2f677d512f54abead4aac74d8114513",
            "199e3d08cab147299018d22638538621",
            "73cf2647d23b4b9b93e3fea78b3cad79",
            "09b4509a1d9345bba31205a480ce94a0",
            "00b11930c1ae4269a22c288f8e3592e5",
            "fa5647222ab041e6a1735ebdde807673",
            "198e5b9856854ed3a12e65cb1a83f7a4",
            "b6c08d7a40454884b5e9500781729608",
            "a0b67fdcd8674ef2b353e27f4f53d3a1",
            "0a134793dea8470b80adbd60d6d3b5cc",
            "06d695e92bc8430c95f87b70321c645a",
            "b1a97616293e4d3ebd4d1c2904f5ae6c",
            "6c9a905cd6e54afaac6f8a28a6a60aa7",
            "bd74b9c7c34e48d085da328d9726a465",
            "4112f6dd93184db382dea0d7bcaae668",
            "52b12f9bf60a44da854fdf32899af8b4",
            "7536da8630c24abda0d98a66af322510",
            "707bab744b7a4994b87901d3f1e1c659",
            "cc1a5c956ff84c36b69cb50a90e84abc",
            "2fbfdd6396604cb1a1ec199ca2c9585a",
            "6f3fd9d7dfac4293bc9bb4df4f736ae2",
            "a0f1b4dc29924a34963293c5e5655218",
            "6fa39eba117d4c9b97b9fa83d7c541d8",
            "1792465c637640c7a83976e2e617a1f7",
            "4b82276bb5684a268a9527997995c4da",
            "efe032f5760c4dc8989a5ac868581494",
            "f755eb91f62041e7afda1d1ff42ba506",
            "d65bcc32daa04aa99f921d3ed85e461e",
            "872d6656760f4cb28ee9e6a78c457599",
            "70359f5c06884ce29cea1a99c0d1ef48",
            "57159588e39540d48875e7fb1f82f979",
            "67fe364796794af3bab0583ab6018b1f",
            "d6e41e6412a7427cbb10180c11f70ccb",
            "d19a5462de0f4c33b3d19b1eea7826a0",
            "3a24177de78e432f8abb753c3cba6bf1",
            "e49f84e1c4ca43e1b1958a31cf65ee2f",
            "3b2098f3590f48e4b3ec579c5ef232e6",
            "e1b075cc90f44e759440f15d9992e0e0",
            "ed8aca8075744e8293d195fd28deb63f",
            "cdbf391617784ac29bba3a77b0787812",
            "3f98b407cdb34ebbbda89aeac9f23847",
            "acd0fc80dd664e2bb270d3558ea48c9d",
            "47ba5f5eaba94f69a9cbf028dd15655c",
            "6b636415e0d24588a4c5083d23fae43d",
            "eed99e9e9a2f4ef49a35ab3732eaa438",
            "33cfa7a37b2e4681970054c0dc33215a",
            "00bc9837349447c69e5851d52172e05d",
            "21a84d5ccbbb401fa30636a18945d61c",
            "4f71cab9f21340b483dc3edd46e994b2",
            "e885b91d2d11400d89af4f334282ad7a",
            "27d70278874c45a4bd147439eeaa4e65",
            "c65cc81f4c674ff3871509d4e047a29e",
            "0c2d79b5277441a09f5dcbcbcf9a4a83",
            "9648934d0cef4bd9a3030387c14158ac",
            "1f0f71e901004c70adbb5a4f5c6c7dd7",
            "18968a3037114fd79ed3f10c3d1f11d5",
            "b64c675de266414eb3a94c9f0bd1effc",
            "b98afa9af4314555bf67d9101e70fc3c",
            "08b62afe872c4f93a320c5981ba5293c",
            "f01b57a812f7481bbee0a642ffe28c46",
            "8e0b7aa4fd234bd8a4ae56f0bb827576",
            "bd03931224c64e9b880a906ee6149a9c",
            "3e470a65a5ce4def965c79fcdce40bad",
            "2c5d6507d2df473bb5c7573ed8c1daa7",
            "edc98b077c044da7957d0751d09562f9",
            "be4b6efbe431448fb61fedecd4d76e34",
            "a2d925ea9a1341e6bb95e7174d9ef539",
            "7cc6220acf174d609eedb1b3a33c97c7",
            "088e7921455644b29303b579f3e16420",
            "244aa9474984491d898c546f6e56eb52",
            "d8976c12e3254794a8f6da1af47d6d2f",
            "aaa338789fe8489789831ee2273e395e",
            "5572dd023e6b4dbbb674ebf0e9c94c3c"
          ]
        },
        "outputId": "784ea912-3645-43c6-b1eb-28bbdc7e6d72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46213848b23c4ec893afcb7b740cffeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c08d7a40454884b5e9500781729608"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc1a5c956ff84c36b69cb50a90e84abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70359f5c06884ce29cea1a99c0d1ef48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f98b407cdb34ebbbda89aeac9f23847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c65cc81f4c674ff3871509d4e047a29e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e470a65a5ce4def965c79fcdce40bad"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We are ready to generate text now"
      ],
      "metadata": {
        "id": "S19CqTX-grUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a text prompt, converts it into model-readable tokens on your GPU, generates a continuation using sampling, decodes it back into text, and prints the result."
      ],
      "metadata": {
        "id": "go4j_nmCjSyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt):\n",
        "  encoded = tokenizer.encode(\n",
        "      prompt,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors = \"pt\"\n",
        "      ).to(device) #encode string as integers so model can actually understand, add special tokens true will auto take care of eos tokens, padding etc, return format compatibile with pytorch. use gpu\n",
        "  out = model.generate(\n",
        "      input_ids=encoded,\n",
        "      repetition_penalty=2.0,\n",
        "      do_sample=True\n",
        "      ) #simulate sampling algo so responses generated are more interesting\n",
        "  string_decoded = tokenizer.decode(out[0].tolist(), clean_up_tokenization_spaces=True) #we then decode the output so we can understand it\n",
        "  print(string_decoded)"
      ],
      "metadata": {
        "id": "_5pO2z3yTr1t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('this is')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezuABdU_gvTg",
        "outputId": "437164f0-d090-4391-b20a-6adca7210efc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a very cool product and I would love to hear your feedback!\n",
            "So far, it has been my goal for the past few weeks. My first game was an old-school strategy RPG with lots of fun elements in mind but also some really interesting gameplay mechanics that were not part or parcel of this project (I hope you enjoyed them!). The story takes place during World War II where all four main heroes have their own army on board together - they are determined by one another's interests as well so there will be many different types available throughout each campaign which could make things even more challenging if we decided at any time between now and September 1st 2017:<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('how are you')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imntdwdGiy6m",
        "outputId": "e5345531-4d78-4cea-abeb-fea6ea5b714b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you going to be in the first place?※<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If I ask it how are you to ChatGPT right now it will answer me: I am good, but I am a LLM with no feelings...\n",
        "\n",
        "Right now with gpt2 its just completing the sentence and generating more text.\n",
        "\n",
        "Now we need to fine tune our model so it functions as a chatbot."
      ],
      "metadata": {
        "id": "865W-FCCkQio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# set up your training arguments\n",
        "training_args = SFTConfig(\n",
        "gradient_accumulation_steps=1,\n",
        "num_train_epochs=1, #how many times do we want to train the model over the entire dataset. Fine tuning does not require a lot\n",
        "learning_rate=2e-4,\n",
        "fp16=True,\n",
        "output_dir=\"logs\",\n",
        "lr_scheduler_type=\"cosine\",# a dynamic learning rate alpha is best, follow cosine pattern\n",
        "warmup_ratio=0.05, #warm up the learning rate\n",
        "group_by_length=True,\n",
        "max_length=512\n",
        "\n",
        ")\n",
        "\n",
        "# initialize the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "model=model,\n",
        "train_dataset=training_dataset,\n",
        "processing_class=tokenizer ,\n",
        "args=training_args,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "c334341c5ccb4d87aadeda5e1b847398",
            "7dd2f1c4a58d40bd89cc0107e13613df",
            "56cf4b4ed17b47e1b704fb17ce774111",
            "6abde831ecf1452fa126d41a0b31fbdb",
            "187eba55b87f461098637daaf79498c0",
            "59ae09036350425694771f671de549fa",
            "b5a495257d5741db8a1cc8916da7c366",
            "2f0db22af17b441193372a46f40f767b",
            "ed9f3fc09998482fa7492ee6c59c44b8",
            "7df5cc99781641628fb62f070045e596",
            "dca1f80dea5449a9ac22833c0079f2d0",
            "b6aca9ad185a4ae78b9c9f9603810aa9",
            "03027135428b432188d855ec48b7485d",
            "8a6a9e4658b94410b6c457e236807d3e",
            "5a430e315b16496d9f784ff11fcd4caf",
            "b6becd226ef549a1b3199000d82262fa",
            "a5e32826dbe045d7982e067b56389d38",
            "211cbb923c3f4c758fa9feb11f3eccd2",
            "7626c55c8335448cb9d4746bcc6fb16d",
            "8c423668d1df4d688a5a39a2880a7c82",
            "9243ca01ec424b1a955228b9a65423bb",
            "229846e98fc94b6cb2ba2c8a7607457a",
            "37936e1bbac34b648f8ef30d55bbb31b",
            "5d04df65c69449afaa1dbb94dbd4bc21",
            "759f27beb2c84f6dbfc0518f470844f0",
            "a5586612ddd94f928ceb941b553d90ef",
            "13b3bf461ca54b759dd0b9c78628b515",
            "12cca3e6764747acbd99a89b2c3bdf9f",
            "026b630b25b44f3da178acac46df7723",
            "43067f03c99545bba04d36eea29a8bb3",
            "805c0444854048ad855385f574d34679",
            "873256ae2b084eba93689a192b0a07e6",
            "253d762d9d8644bda10f529f91d804de"
          ]
        },
        "id": "NQ2ZGZLdwozY",
        "outputId": "5840979c-b596-4aac-85c9-c0038a264a9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c334341c5ccb4d87aadeda5e1b847398"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6aca9ad185a4ae78b9c9f9603810aa9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1229 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37936e1bbac34b648f8ef30d55bbb31b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "fG4dR-g0wgoa",
        "outputId": "e2d32826-07bf-47ab-957d-064a94ac8303"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Invalid API key: API key must have 40+ characters, has 1.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260108_224102-2nm2qq8z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:44, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.309400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.334700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.264500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.435200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.295800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.298600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.156200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.236600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.385200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m URL not available in offline run\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=125, training_loss=3.325929244995117, metrics={'train_runtime': 307.0327, 'train_samples_per_second': 3.257, 'train_steps_per_second': 0.407, 'total_flos': 94701699661824.0, 'train_loss': 3.325929244995117, 'entropy': 3.4376644611358644, 'num_tokens': 363618.0, 'mean_token_accuracy': 0.3801665484905243, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('how are you')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdx-g_FH3R2h",
        "outputId": "ecd120ee-9d13-41b2-e545-3d9cdd66359f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you?\n",
            " (). for that<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3aQo_dpB7FV3"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}